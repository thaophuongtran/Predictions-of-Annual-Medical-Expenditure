# -*- coding: utf-8 -*-
"""Predictions of Annual Medical Expenditure.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14TAtBoN0h1lPzQHFjuiougnotma1JkSe

Predictions of Annual Medical Expenditure
---

Thao Tran

## Introduction

In this anlysis, we use neural networks to provide predictions on annual insurance premium (amount paid every month) offered to the customer based on their age, sex, BMI, children, smoking habits and region of residence. The goal is to estimate annual medical expenditure for new customers using the above measures. Our data of over 1300 customers comes from Kaggle, ["Medical Insurance Payout"](https://www.kaggle.com/datasets/harshsingh2209/medical-insurance-payout?resource=download), with the following variables:


* `age`: Age of the customer
* `sex`: Gender
* `bmi`: Body Mass Index, an important health factor
* `children`: Number of children
* `smoker`: Whether the customer smokes or not
* `region`: Region of the country the customer resides
* `charges`: The annual insurance expenditure for the customer
"""

#load library
import numpy as np
import pandas as pd
import sklearn.model_selection
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score
#plots
import seaborn as sns
import matplotlib.pyplot as plt
# Keras
import keras
import tensorflow as tf
from keras.layers import Dense
from keras.models import Sequential

#load data
med = pd.read_csv("expenses.csv")
med.head()

"""## Exploratory Data Analysis

We have 1338 records of customers, who have an average age of 39 and bmi of 31. There is an even distrbution of observations across sex and four regions, and about a fifth of customer records are smokers.
"""

med.describe().transpose()

med.describe(include = ['object'])

"""Our target variable, `charges`, is unimodal and right-skewed with an average of \$13,270.42 and standard deviation of \$12,110.01. While the annual medical expenditure ranges from \$1,121.87 and \$63,770.43, its distribution can be distinguished between smokers (higher charges) and non-smokers (lower charges). The box plots of charges by `smoker` also shows that the mean expenses for smokers are significantly higher than the mean expenses for non-smokers. """

sns.histplot(data=med, x="charges",kde=True, hue = "smoker")

sns.boxplot(x="smoker", y="charges", data=med)

"""We also explore the relationship between `charges` and other variables and whether they are indicative of insurance payout in addition to `smoker`. The scatter plot between `age` and `charges` shows that there is an increasing trend where people at higher age experience higher expenses. The points form three increasing trends at different levels of charges. While `smoker` does help explain seprate some of these levels, there is other underlying patterns in place that we need to explore. """

sns.scatterplot(data=med, x="age", y="charges",hue = "smoker")

"""Body mass index, `bmi`, is a person's weight divided by the square of height in meters. It is used as a screening tool for obesity and an important indicator for various metabolic and disease outcomes. In our analysis, we do not observe a linear relationship between `bmi` and `charges` however there are identifiable clusters. While bmi is not indicative of expenses for non-smokers whose records under \$15,000. Among smokers, those with bmi under 30 have medical payout between \$15,000 and \$30,000 and those with bmi above 30 have payout more than \$30,000.  """

sns.scatterplot(data=med, x="bmi", y="charges",hue = "smoker")

"""As we further investigate with box plots of medical expenses by sex, number of children, and regions, we find no clear evidence that these variable are predictive of `charges`."""

sns.boxplot(x="sex", y="charges", data=med)

sns.boxplot(x="children", y="charges", data=med)

sns.boxplot(x="region", y="charges", data=med)

"""## Neural Networks Using Keras

We proceed to employ simple shallow and deep neural networks on six independent variables to predict `charges`. We use keras, a popular deep learning package which enables rapid implementation of neural networks.
"""

#convert object to numeric 
med['sex'] = pd.factorize(med.sex)[0]
med['smoker'] = pd.factorize(med.smoker)[0]
med['region'] = pd.factorize(med.region)[0]

#scaler
scalar = MinMaxScaler()
scalar.fit(med['charges'].values.reshape(-1, 1))
# split data into training and testing sets
y = scalar.transform(med['charges'].values.reshape(-1, 1))
x = med.drop('charges',axis=1)
x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y,random_state = 0, test_size = 0.2)

"""### Activation Functions

We experiment with different activation functions

* relu: Rectified Linear Unit Function
* tanh: Hyperbolic Tangent Function
* swish: Smooth, Non-Monotonic Function
* gelu: Gaussian Error Linear Unit Function

We used r-squared to compare the performance of each model and mean squared error loss function given our objective is a regression problem. With batch size of 100 and 150 epochs, we find that gelu function performs the best among the four with r-square above 86 percent.
"""

#keras model 
keras_model = keras.models.Sequential([keras.layers.Dense(6, input_dim=6, activation='relu'),
                                 keras.layers.Dense(6, activation='relu'),
                                 keras.layers.Dense(1, activation='sigmoid')])
keras_model.compile(loss="mean_squared_error", optimizer="adam")
keras_model.fit(x_train, y_train, batch_size =100, epochs=150)
y_predict = keras_model.predict(x_test)
#r squared
r2_score(y_test, y_predict)

#keras model 
keras_model = keras.models.Sequential([keras.layers.Dense(6, input_dim=6, activation='tanh'),
                                 keras.layers.Dense(6, activation='tanh'),
                                 keras.layers.Dense(1, activation='sigmoid')])
keras_model.compile(loss="mean_squared_error", optimizer="adam")
keras_model.fit(x_train, y_train, batch_size =100, epochs=150)
y_predict = keras_model.predict(x_test)
#r squared
r2_score(y_test, y_predict)

#keras model 
keras_model = keras.models.Sequential([keras.layers.Dense(6, input_dim=6, activation='swish'),
                                 keras.layers.Dense(6, activation='swish'),
                                 keras.layers.Dense(1, activation='sigmoid')])
keras_model.compile(loss="mean_squared_error", optimizer="adam")
keras_model.fit(x_train, y_train, batch_size =100, epochs=150)
y_predict = keras_model.predict(x_test)
#r squared
r2_score(y_test, y_predict)

#keras model 
keras_model = keras.models.Sequential([keras.layers.Dense(6, input_dim=6, activation='gelu'),
                                 keras.layers.Dense(6, activation='gelu'),
                                 keras.layers.Dense(1, activation='sigmoid')])
keras_model.compile(loss="mean_squared_error", optimizer="adam")
keras_model.fit(x_train, y_train, batch_size =100, epochs=150)
y_predict = keras_model.predict(x_test)
#r squared
r2_score(y_test, y_predict)

"""### Optimizer

We also experiment with different optimizer:

* sgd: Stochastic Gradient Descent
* rmsprop: Root Mean Squared Propagation
* adam: Adaptive Moment Estimation

We find that the adam optimizer perform the best among the three optimizers. Thus we proceed to a deeper architecture with gelu activation function and adam optimizer.
"""

#keras model 
keras_model = keras.models.Sequential([keras.layers.Dense(6, input_dim=6, activation='gelu'),
                                 keras.layers.Dense(6, activation='gelu'),
                                 keras.layers.Dense(1, activation='sigmoid')])
keras_model.compile(loss="mean_squared_error", optimizer="sgd")
keras_model.fit(x_train, y_train, batch_size =100, epochs=150)
y_predict = keras_model.predict(x_test)
#r squared
r2_score(y_test, y_predict)

#keras model 
keras_model = keras.models.Sequential([keras.layers.Dense(6, input_dim=6, activation='gelu'),
                                 keras.layers.Dense(6, activation='gelu'),
                                 keras.layers.Dense(1, activation='sigmoid')])
keras_model.compile(loss="mean_squared_error", optimizer="rmsprop")
keras_model.fit(x_train, y_train, batch_size =100, epochs=150)
y_predict = keras_model.predict(x_test)
#r squared
r2_score(y_test, y_predict)

#keras model 
keras_model = keras.models.Sequential([keras.layers.Dense(6, input_dim=6, activation='gelu'),
                                 keras.layers.Dense(6, activation='gelu'),
                                 keras.layers.Dense(1, activation='sigmoid')])
keras_model.compile(loss="mean_squared_error", optimizer="adam")
keras_model.fit(x_train, y_train, batch_size =100, epochs=150)
y_predict = keras_model.predict(x_test)
#r squared
r2_score(y_test, y_predict)

"""### Deeper Architecture

While we experiment with different components of the neural networks setup, we employ a deeper architecture and observe whether this would improve the prediction performance. While there is a slight improvement on the r-squared at 87 percent, it is not significantly better than the shallow model.
"""

#keras model 
keras_model = keras.models.Sequential([keras.layers.Dense(6, input_dim=6, activation='gelu'),
                                 keras.layers.Dense(6, activation='gelu'),
                                 keras.layers.Dense(5, activation='gelu'),
                                 keras.layers.Dense(4, activation='gelu'),
                                 keras.layers.Dense(6, activation='gelu'),
                                 keras.layers.Dense(5, activation='gelu'),
                                 keras.layers.Dense(4, activation='gelu'),
                                 keras.layers.Dense(1, activation='sigmoid')])
keras_model.compile(loss="mean_squared_error", optimizer="adam")
keras_model.fit(x_train, y_train, batch_size =100, epochs=150)
y_predict = keras_model.predict(x_test)

from sklearn.metrics import r2_score
r2_score(y_test, y_predict)

#error 
sns.histplot((y_test-y_predict),kde=True)

#absolute error
abs_err = abs(y_test-y_predict)
sns.histplot(abs_err,kde=True)

#squared error
sqr_err = (y_test-y_predict)**2
sns.histplot(sqr_err,kde=True)

"""**How many epochs are enough?**



"""

# Fit the model
history = keras_model.fit(x, y, validation_split=0.2, batch_size =100, epochs=1000)
# list all data in history
print(history.history.keys())
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""**Visualize the deep models architecture**"""

from keras.utils.vis_utils import plot_model

#plot_model(word2vec_model, to_file='word2vec_model_plot.png', show_shapes=True, show_layer_names=True)
plot_model(keras_model, show_shapes=True, show_layer_names=True)

"""### Variables Importance"""

!pip install eli5

import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(keras_model, random_state=1,scoring = 'r2').fit(x,y)
eli5.show_weights(perm, feature_names = x.columns.tolist())

sns.barplot(y = perm.feature_importances_ , x = x.columns.tolist())

"""Lastly, we use the `eli5` package to explore the variable importance in our model performance. Unsurprisingly, we find `smoker`, `bmi`, and `age` to be the most important variables in predicting annual medical expenses. """